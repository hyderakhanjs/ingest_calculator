{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"17wVTwl27oIFCrvdWV4ep7ZOmsnrhR4aA","authorship_tag":"ABX9TyPStlaDeaDQiT11KZ0PYdfo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"aPnlDnecq_pY","executionInfo":{"status":"ok","timestamp":1707435661562,"user_tz":360,"elapsed":26261,"user":{"displayName":"Hyder Khan","userId":"04747023059777142748"}}},"outputs":[],"source":["import pandas as pd\n","import json\n","import re\n","\n","#the location of your input files. CHANGE BEFORE RUNNING\n","INPUT_FILE_LOC = '/content/drive/MyDrive/ryder diag'\n","\n","\n","# Following is required for writing the output. CHANGE BEFORE RUNNING\n","OUTPUT_FILE_LOC = '/content/drive/MyDrive/ryder diag'\n","\n","\n","\n","with open(f\"{INPUT_FILE_LOC}/ilm_policies.json\") as f:\n","  ilm_data = json.load(f)\n","\n","with open(f\"{INPUT_FILE_LOC}/indices.json\") as f:\n","  indices_data = json.load(f)\n","\n","def extract_num_days(str):\n","\n","  pattern = re.compile(r\"\\d+\")\n","  extr = pattern.search(str)\n","  if extr:\n","    return int(extr.group())\n","  else:\n","    return 0\n","\n","def keys_exists(element, *keys):\n","    '''\n","    Check if *keys (nested) exists in `element` (dict).\n","    '''\n","    if not isinstance(element, dict):\n","        raise AttributeError('keys_exists() expects dict as first argument.')\n","    if len(keys) == 0:\n","        raise AttributeError('keys_exists() expects at least two arguments, one given.')\n","\n","    _element = element\n","    for key in keys:\n","        try:\n","            _element = _element[key]\n","        except KeyError:\n","            return False\n","    return True\n","\n","index_df = pd.DataFrame(columns=['index_name', 'store', 'docs', 'node', 'prirep'])\n","\n","for index in indices_data:\n","  index_df.loc[len(index_df)] = [index.get('index'),index.get('store'), index.get('docs'), index.get('node'), index.get('prirep')]\n","\n","#index_df.to_csv()\n","\n","\n","\n","df = pd.DataFrame(columns=['policy', 'index_name', 'hot', 'warm', 'cold', 'frozen'])\n","\n","\n","\n","for item in ilm_data:\n","  ilm_data_row = ilm_data.get(item)\n","  hot = 0\n","  hot_max = 0\n","  warm = 0\n","  cold = 0\n","  frozen = 0\n","  delete = 0\n","\n","  if ilm_data_row.get('policy').get('phases').get('hot'):\n","    #hot_str = ilm_data_row.get('policy').get('phases').get('hot').get('min_age')\n","    if keys_exists(ilm_data_row,'policy','phases', 'hot', 'actions','rollover','max_age'):\n","      hot_max_age_str = ilm_data_row.get('policy').get('phases').get('hot').get('actions').get('rollover').get('max_age')\n","      #hot = extract_num_days(hot_str)\n","      hot_max = extract_num_days(hot_max_age_str)\n","\n","  if ilm_data_row.get('policy').get('phases').get('warm'):\n","    warm_str = ilm_data_row.get('policy').get('phases').get('warm').get('min_age')\n","    warm = extract_num_days(warm_str)\n","\n","\n","  if ilm_data_row.get('policy').get('phases').get('cold'):\n","    cold_str = ilm_data_row.get('policy').get('phases').get('cold').get('min_age')\n","    cold = extract_num_days(cold_str)\n","\n","\n","  if ilm_data_row.get('policy').get('phases').get('frozen'):\n","    frozen_str = ilm_data_row.get('policy').get('phases').get('frozen').get('min_age')\n","    frozen = extract_num_days(frozen_str)\n","\n","  if ilm_data_row.get('policy').get('phases').get('delete'):\n","    delete_str = ilm_data_row.get('policy').get('phases').get('delete').get('min_age')\n","    delete = extract_num_days(delete_str)\n","\n","\n","  if warm > 0:\n","    hot = warm\n","  elif cold > 0:\n","    hot = cold\n","  elif frozen > 0:\n","    hot = frozen\n","  else:\n","    hot = hot_max\n","\n","  if cold > 0:\n","    warm = cold - hot\n","  elif frozen > 0:\n","    warm = frozen - (cold+hot)\n","\n","  if frozen > 0:\n","    frozen = delete - (hot+warm+cold)\n","\n","\n","\n","\n","\n","\n","\n","  in_use_by_indices = ilm_data_row.get(\"in_use_by\").get(\"indices\")\n","  for index_name in in_use_by_indices:\n","    df.loc[len(df.index)] = [item,index_name,hot,warm,cold,frozen]\n","\n","\n","df[['hot','warm','cold','frozen']] = df[['hot','warm','cold','frozen']].apply(pd.to_numeric)\n","\n","\n","merged_pd = pd.merge(df,index_df,on='index_name')\n","\n","merged_pd.to_csv(f\"{OUTPUT_FILE_LOC}/merged_indices.csv\")\n","\n"]},{"cell_type":"code","source":["GB = 1000000000\n","primary_df = merged_pd.loc[merged_pd['prirep'] == 'p']\n","#primary_df = merged_pd\n","primary_df[['store', 'docs']] = primary_df[['store', 'docs']].apply(pd.to_numeric)\n","\n","grouped_df = primary_df.groupby([\"policy\",\"hot\",\"warm\",\"cold\",\"frozen\"]).agg({'index_name':'count', 'store': 'sum', 'docs' : 'sum'})\n","\n","store_in_hot_bytes_list = []\n","store_in_warm_bytes_list = []\n","store_in_cold_bytes_list = []\n","store_in_frozen_bytes_list  = []\n","daily_ingest_bytes_list = []\n","store_in_gb_list = []\n","\n","for item in grouped_df.iterrows():\n","  hot = item[0][1]\n","  warm = item[0][2]\n","  cold = item[0][3]\n","  frozen = item[0][4]\n","  store = item[1][1]\n","\n","  hot_days_ratio = 0\n","  warm_days_ratio = 0\n","  cold_days_ratio = 0\n","  frozen_days_ratio = 0\n","  daily_ingest_bytes = 0\n","  store_in_hot_bytes = 0\n","  total_days = hot + warm + cold #+ frozen\n","\n","\n","  if total_days > 0:\n","    hot_days_ratio = hot / total_days\n","    warm_days_ratio = warm / total_days\n","    cold_days_ratio = cold / total_days\n","    frozen_days_ratio = frozen / total_days\n","\n","\n","  store_in_hot_bytes = round(((store * hot_days_ratio)/GB),4)\n","  store_in_warm_bytes = round(((store * warm_days_ratio) / GB),4)\n","  store_in_cold_bytes = round(((store * cold_days_ratio) / GB),4)\n","  store_in_frozen_bytes = round(((store * frozen_days_ratio) / GB),4)\n","  store_in_gb = round((store/GB),4)\n","\n","\n","\n","  if(hot > 0):\n","    daily_ingest_bytes = round((store_in_hot_bytes / hot),4)\n","\n","  store_in_hot_bytes_list.append(store_in_hot_bytes)\n","  store_in_warm_bytes_list.append(store_in_warm_bytes)\n","  store_in_cold_bytes_list.append(store_in_cold_bytes)\n","  store_in_frozen_bytes_list.append(store_in_frozen_bytes)\n","  daily_ingest_bytes_list.append(daily_ingest_bytes)\n","  store_in_gb_list.append(store_in_gb)\n","\n","grouped_df.insert(len(grouped_df.columns),column=\"storage(gb)\",value=store_in_gb_list)\n","grouped_df.insert(len(grouped_df.columns),column=\"hot(gb)\",value=store_in_hot_bytes_list)\n","grouped_df.insert(len(grouped_df.columns),column=\"warm(gb)\",value=store_in_warm_bytes_list)\n","grouped_df.insert(len(grouped_df.columns),column=\"cold(gb)\",value=store_in_cold_bytes_list)\n","grouped_df.insert(len(grouped_df.columns),column=\"frozen(gb)\",value=store_in_frozen_bytes_list)\n","grouped_df.insert(len(grouped_df.columns),column=\"daily_ingest(gb)\",value=daily_ingest_bytes_list)\n","grouped_df.drop(columns=[\"store\"], inplace=True)\n","grouped_df.rename(columns={\"index_name\" : \"indices\"},inplace=True)\n","\n","grouped_df.to_csv(f\"{OUTPUT_FILE_LOC}/merged_ilm.csv\")\n","\n","\n","grouped_df"],"metadata":{"id":"mfFGJgeylUlS"},"execution_count":null,"outputs":[]}]}